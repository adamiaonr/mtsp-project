\section{Methodology}
\label{sec:methodology}

The work presented here follows the consulted background literature about 
supervised and semi-supervised text classification~\cite{McCallum98acomparison,Nigam2000,Rennie03tacklingthe}, 
reflecting our understanding of the material.
%reflecting our understanding of the material.\vertbreak

%We start by introducing some of the notation used in the rest of the paper, and 
%then continue with the description of a fully supervised generative model for 
%text classification --- Multinomial Naive Bayes --- followed by semi-supervised 
%extensions to that model, which use the Expectation Maximization (EM) algorithm.

\subsection{Notation}

Text classification mainly deals with categorizing a set of text articles into 
some topic, according to a set of features such as the words contained in them. 
Here we present the notation to formalize these concepts.\vertbreak

An article $a$, 
belonging to a topic (i.e. a label\slash class) $t$, is represented by an 
array $a = \{w_{1}, w_{2}, ..., w_{|\mathcal{D}|}, t\}$, i.e. a list of 
$|\mathcal{D}|$ features and its label. In this case, each feature typically 
corresponds to the number of occurrences of the word $w_{i}$, the $i$-th word in a 
dictionary $\mathcal{D}$, within an article $a_i$. Other options for 
representing word frequency exist, such as Term Frequency and 
Inverse Document Frequency (TF-IDF)~\cite{Rennie03tacklingthe}.\vertbreak

As an example, consider an article set composed by two instances, 
$\mathcal{A} = \{a_1, a_2\}$, each one of them with the following contents:
\[a_1 : \verb+You like potato, I like potato.+\]
\[a_2 : \verb+I say tomato, you say tomato.+\]
In this case, the dictionary $\mathcal{D}$ would contain the following 
words~\footnote{As we will see in Section~\ref{subsec:exp-setup}, in practice, the 
words in $\mathcal{D}$ might be converted to a common format, e.g. all words 
converted to lowercase, stripped of punctuation, etc.}:
\[\mathcal{D} = \{\verb+You+, \verb+like+, \verb+potato+, \verb+I+, \verb+say+, \verb+tomato+\}\]
The articles $a_1$ and $a_2$ could then be codified (using the sparse 
`bag-of-words' approach~\cite{Su2011}) as:
\[a_1 = \{1, 2, 2, 1, 0, 0\}\]
\[a_2 = \{1, 0, 0, 1, 2, 2\}\]

\subsection{Generative Models for Text Classification}
\label{subsec:gen-models}

McCallum et al.~\cite{McCallum98acomparison} present two 
generative models for text classification, (1) a Multivariate Bernoulli event 
model and (2) a Multinomial event model: the first case considers multivariate 
Bernoulli as the parametric distributions describing each mixture component, 
only capturing the (non-)\,occurrence of word events in articles, while the 
second case considers Multinomial distributions, now capturing the quantity of 
word events. The authors state that the multinomial event model generally 
outperforms the multivariate Bernoulli model, specially when considering 
large dictionary sizes~\cite{McCallum98acomparison}. For the remainder of 
this paper, we focus our attention on the Multinomial event model.\vertbreak

Despite our focus on the Multinomial model, being GMs, both models assume (1) that an article $a$ is generated 
according to a mixture model, encompassing several mixture components $c_j \in 
\mathcal{C} = \{c_1, c_2, ..., c_{|\mathcal{C}|}\}$. The shape of each component 
distribution is governed by a set of parameters $\theta$. It is also assumed (2) there is a 1:1 
correspondence between the components in $\mathcal{C}$ and topics of articles 
$t_i$.\vertbreak

One can look at the process of 
`generating' an article $a_i$ in the following manner: (1) Selecting a 
component $c_j$ from the mixture model, with probability $P(c_j|\theta)$; (2) 
letting $c_j$ generate $a_i$ according to its own distribution 
$P(a_i|c_j,\theta)$. This results in the probability of an article $a_i$ being 
generated by a component $c_j$:
\begin{equation}
    P(c_j|\theta)P(a_i|c_j,\theta)
    \label{eq:data-component}
\end{equation}

As different components in $\mathcal{C}$ can contribute to origin a similar 
article $a_i$, the probability of finding an article $a_i$ is obtained by 
marginalizing expression~\ref{eq:data-component} over all the components in 
$\mathcal{C}$:
\begin{equation}
    P(a_i|\theta) = \sum_{j=1}^{|\mathcal{C}|}P(c_j|\theta)P(a_i|c_j,\theta)
    \label{eq:data-mixture}
\end{equation}

\subsection{Multinomial Naive Bayes}
\label{subsec:multinomial-naive}

The Multinomial Naive Bayes (MNB) model described by McCallum et al. 
in~\cite{McCallum98acomparison} captures the frequency of words in articles and 
applies a `bag-of-words' approach for article representation.\vertbreak 

The second step in the process of generating an article $a_i$ --- related to the 
$P(a_i|c_j,\theta)$ term in expression~\ref{eq:data-component} --- can be further 
expressed as a sequence of $|a_i|$ draws (with replacement) of words $w_k$ from 
the dictionary $\mathcal{D}$. Besides the assumptions mentioned in 
Section~\ref{subsec:gen-models}, the MNB model considers 
that (1) the length of an article $|a_i|$ (word count) is independent of the 
topic\slash mixture model component $c_j$; and (2) the appearances of words in 
an article are conditionally independent from each other, given an article 
topic: the so-called `Naive Bayes' assumption. The class conditional 
probability of an article $a_i$ can then be thought 
of as a multinomial distribution over words, with $a_i$ independent trials, in 
the form:
\begin{equation}
\begin{split}
    P(a_i|c_j,\theta) &= |a_i|!\prod_{k=1}^{|\mathcal{D}|}\frac{P(w_{k}|c_j,\theta)^{N_{i,k}}}{N_{i,k}!}\\
                &\propto \prod_{k=1}^{|\mathcal{D}|}P(w_{k}|c_j,\theta)^{N_{i,k}}
    \label{eq:data-naive}
\end{split}
\end{equation}

where $N_{i,k}$ is the number of times word $w_k$ of a dictionary $\mathcal{D}$ 
appears in an article $a_i$. In practice, as the multinomial coefficient 
$\frac{|a_i|!}{N_{i,1}! ... N_{i,|\mathcal{D}|}!}$ does not depend on the 
mixture components $c_j$, it is often ignored when the purpose is to maximize 
the likelihood $P(a_i|c_j,\theta)$~\cite{Nigam2000, Kibriya:2004:MNB:2146834.2146882, Su2011}.\vertbreak

The set of mixture model parameters $\theta$ to be estimated 
during the training phase of the classifier consists of (1) each of the class 
conditional probabilities of words 
$\hat{\theta}_{w_k|c_j} \equiv P(w_k|c_j,\hat{\theta})$; and (2) the prior 
probabilities for each topic\slash mixture model 
component $\hat{\theta}_{c_j} \equiv P(c_j|\hat{\theta})$.\vertbreak 

These estimates 
are obtained by Maximum a Posteriori (MAP) estimation of the parameters, i.e. 
we find the values of $\theta$ that maximize the posterior probability 
$P(\theta|\mathcal{A})$: 
\begin{equation}
\begin{split}
    %P(\theta|\mathcal{A}) &= \sum_{j=1}^{|\mathcal{C}|}P(c_j,\theta) \times \prod_{i=1}^{|\mathcal{A}|}P(a_i|\theta)\\
    P(\theta|\mathcal{A}) &\propto P(\theta) \times P(\mathcal{A}|\theta)
    %&\propto \prod_{j=1}^{|\mathcal{C}|} \left(\theta_{c_j}^{\sigma - 1} \prod_{k=1}^{|\mathcal{D}|} \theta_{w_k|c_j}^{\sigma - 1}\right)  \times \prod_{i=1}^{|\mathcal{A}|}P(a_i|\theta)
    \label{eq:posterior-map}
\end{split}
\end{equation}

Here we follow the same representation of $P(\theta)$ used by 
Nigam et al.~\cite{Nigam2000}, a Dirichlet distribution, with $\sigma = 2$. 
The probability $P(\mathcal{A}|\theta)$ is equal to 
$\prod_{i=1}^{|\mathcal{A}|}P(a_i|\theta)$, as one assumes the article 
generation events are independent between each other, given the mixture model 
parameters $\theta$.\vertbreak

The results of the MAP 
estimation reduce to `counting problems'. 
Specifically, $\hat{\theta}_{w_k|c_j}$ is given by the ratio of appearances of 
a word $w_k$ within all articles $a_i$ belonging to a component $c_j$ vs. the 
total number of word events for $c_j$:
\begin{equation}
\begin{split}
    \hat{\theta}_{w_k|c_j} &\equiv P(w_k|c_j,\hat{\theta})\\
    &= \frac{\alpha + \sum_{i=1}^{|\mathcal{A}|}N_{i,k}P(t_i = c_j|a_i)}{\alpha|\mathcal{D}| + \sum_{s=1}^{|\mathcal{D}|}\sum_{i=1}^{|\mathcal{A}|}N_{i,s}P(t_i = c_j|a_i)}
    \label{eq:class-cond-estimate}
\end{split}
\end{equation}

where $P(c_j|a_i) \in \{0,1\}$ depending on the label of an article $a_i$
and $N_{i,k}$ is the number of occurrences of the word $w_k$ in an article 
$a_i$. Notice the inclusion of `smoothing priors' $\alpha$, used to avoid 
probabilities equal to zero in the lack of particular word events for a 
component $c_j$. In~\cite{McCallum98acomparison,Nigam2000} the authors use 
$\alpha = 1$, which is designated by Laplace smoothing.\vertbreak

The parameters $\hat{\theta}_{c_j}$ are given 
by the ratio of the articles belonging to a component $c_j$ vs. the total 
number of articles $\mathcal{A}$:
\begin{equation}
\begin{split}
    \hat{\theta}_{c_j} &\equiv P(c_j|\hat{\theta}) = \frac{\alpha + \sum_{i=1}^{|\mathcal{A}|}P(t_i = c_j|a_i)}{\alpha|\mathcal{C}| + \sum_{i=1}^{|\mathcal{A}|}P(t_i = c_j|a_i)}
    \label{eq:prior-estimate}
\end{split}
\end{equation}

% ONE SHOULD MENTION THAT THESE EXPRESSIONS FOR PARAMETER ESTIMATIONS COME FROM 
% MAP ESTIMATION, I.E. LEARNING A CLASSIFIER IS APPROACHED BY CALCULATING A MAP 
% ESTIMATE OF THE \theta PARAMETERS.

%(...) applying the chain rule of probability~\cite{}, one can extend the second 
%term of Equation~\ref{eq:data-mixture} as follows (still without taking the 
%Naive Bayes assumption into account):

%\begin{equation}
%\begin{split}
%    P(a_i|t_j,\theta) &= P(\{N(w_{1},a_i), ..., N(w_{N},a_i)\}|c_m,\theta)\\
%    &= P(\sum_{n=1}^{N}(N(w_{n},a_i)))
%    \label{eq:data-component-chain}
%\end{split}
%\end{equation}

The final expression for the posterior 
probabilities $P(c_j|a_i,\theta)$, i.e. the probability of the class given an 
article, is obtained via Bayes' Rule:
\begin{equation}
\begin{split}
    P(c_j|a_i,\hat{\theta}) &= \frac{P(c_j|\hat{\theta})P(a_i|c_j,\hat{\theta})}{P(a_i|\hat{\theta})}\\
    &\propto P(c_j|\hat{\theta})P(a_i|c_j,\hat{\theta})
    \label{eq:posterior}
\end{split}
\end{equation}

Expression~\ref{eq:posterior} can be progressively expanded by replacing each 
one of the terms by the corresponding expressions, given in equations 
\ref{eq:data-mixture},~\ref{eq:data-naive},~\ref{eq:class-cond-estimate} and~\ref{eq:prior-estimate}.\vertbreak

Despite its unrealistic 
simplifications and assumptions (data generated by mixture model; 1:1 
correspondence between mixture components and topics; conditional independence 
of word events; article length independence), Naive Bayes classifiers have 
proven to provide fair classification 
performance~\cite{McCallum98acomparison,Nigam2000}. Due to their simplicity, 
these are well suited for text classification tasks, where the 
number of features is usually large (i.e. dictionary sizes often reaching orders 
of thousands of words~\cite{McCallum98acomparison}).

\subsection{Semi-Supervised Learning via Expectation 
Maximization (EM)}
\label{subsec:semi-super-em}

The MNB text classifier described in 
Section~\ref{subsec:multinomial-naive} falls into the scope of 
supervised learning, only taking labeled data into 
account. Despite its fair performance when trained with large amounts of 
labeled data, Nigam et al.~\cite{Nigam2000} notice how it suffers when faced 
with small-sized datasets and show some advantages (regarding 
classification accuracy) of expanding such models to the semi-supervised 
learning scope, i.e. considering both labeled and 
unlabeled data.\vertbreak 

% intro paragraph is ok. now briefly explain how the overall process works, in 
% a sequence of steps.

% now, explain the EM process with more detail, starting with the formula for 
% the probability of the data given the model P(\mathcal{A}|\theta).

As with the case of the MNB classifier, the parameters $\theta$ are obtained 
by MAP estimation, i.e. maximizing expression~\ref{eq:posterior-map}. However, 
we should note that now the training dataset is composed by the labeled and 
unlabeled subsets $\mathcal{A}^{\ell}$ and $\mathcal{A}^{u}$. Following the 
Semi-Supervised GM ideas introduced in Section~\ref{subsec:gen-models}, the expression 
for $P(\mathcal{A}|\theta)$ becomes:
\begin{equation}
\begin{split}
    P(\mathcal{A}|\theta) = &\prod_{a_i \in \mathcal{A}^{u}}\sum_{j=1}^{|\mathcal{C}|}P(c_j|\theta)P(a_i|c_j,\theta)\\ 
    &\times \prod_{a_i \in \mathcal{A}^{\ell}}P(t_i = c_j|\theta)P(a_i|t_i = c_j,\theta)
    \label{eq:data-mixture-em}
\end{split}
\end{equation}

Note that for the set of labeled articles, $\mathcal{A}^{\ell}$, one 
already knows the true topic\slash mixture model component $c_j$ which 
generated each article $a_i$, hence there is not the need of referring to all 
components in $\mathcal{C}$. However, for the unlabeled set $\mathcal{A}^{u}$, 
each component $c_j$ has a contribution to the generation of each article $a_i$ 
which must be taken into account.\vertbreak

As described in~\cite{Nigam2000} (and as with the case of the MNB 
classifier), expression~\ref{eq:data-mixture-em} can be passed to logarithmic 
form, with the maximization of $P(\theta)P(\mathcal{A}|\theta)$ being 
accomplished by solving the system of partial derivatives of 
log$(P(\theta)P(\mathcal{A}|\theta))$. Here we use the same nomenclature as that 
used in~\cite{Nigam2000}, $\ell(\theta|\mathcal{A}) \equiv 
\text{log}(P(\theta)P(\mathcal{A}|\theta))$:
\begin{equation}
\begin{split}
    \ell\left(\theta|\mathcal{A}\right) &= \text{log}\left(P(\theta)\right)\\
        &+ \sum_{a_i \in \mathcal{A}^{u}}\text{log}\left(\sum_{j=1}^{|\mathcal{C}|}P(c_j|\theta)P(a_i|c_j,\theta)\right)\\ 
        &+ \sum_{a_i \in \mathcal{A}^{\ell}}\text{log}\left(P(t_i = c_j|\theta)P(a_i|t_i = c_j,\theta)\right)
    \label{eq:log}
\end{split}
\end{equation}

The log of sums over all mixture components $\mathcal{C}$ for all $a_i \in 
\mathcal{A}^{u}$ makes the problem computationally 
intractable~\cite{Nigam2000}. This is where the EM algorithm comes into play, 
providing an iterative process to obtain a MAP estimation of the parameters 
$\theta$, including the unlabeled data~\cite{Nigam2000}. We now proceed with a brief 
description of the overall MNB + EM process, without detailing the inner works 
of EM (refer to~\cite{Nigam2000} for further details):

\begin{enumerate}

\item Train a MNB classifier with the labeled dataset $\mathcal{A}^{\ell}$ only. 
        Find the estimated parameters $\hat{\theta}$ using 
        expressions~\ref{eq:class-cond-estimate} and~\ref{eq:prior-estimate}.

\item \textbf{EM's E Step:} Use the classifier governed by the current $\hat{\theta}$ parameters to 
        estimate the contribution of each mixture model component $c_j$ to the 
        generation of each article in the unlabeled dataset $\mathcal{A}^{u}$, 
        i.e. determine $P(c_j|a_i,\hat{\theta})$ $\forall$ $a_i \in \mathcal{A}^{u}$ 
        using expression~\ref{eq:posterior}.

\item \textbf{EM's M Step:} Re-estimate the $\hat{\theta}$ parameters at the light of the new 
        $P(c_j|a_i,\hat{\theta})$ for both $\mathcal{A}^{\ell}$ and $\mathcal{A}^{u}$, using 
        expressions~\ref{eq:class-cond-estimate} and~\ref{eq:prior-estimate}. 
        Note that now $P(c_j|a_i,\hat{\theta})$ varies between 0 and 1 for $\mathcal{A}^{u}$ 
        (as opposed to $P(c_j|a_i,\hat{\theta}) \in \{0,1\}$ 
        $\forall$ $a_i \in$ $\mathcal{A}^{\ell}$).

\item Evaluate $\ell(\theta|\mathcal{A})$ using 
        expression~\ref{eq:log}. If $\Delta\ell(\theta|\mathcal{A}) > T$, 
        $T$ being a convergence threshold, return 
        to step 2. Else, accept the classifier governed by the current $\hat{\theta}$ as the final solution.

\end{enumerate}

Nigam et al.~\cite{Nigam2000} note that while this simple MNB + EM combination 
performs well over datasets containing small amounts of labeled data 
vs. large amounts of unlabeled data (e.g. with differences within the 
range of 1000 vs. 10000~\cite{Nigam2000}, in the case of the 20 Newsgroups 
dataset~\cite{Lang95}), it may decrease the classification accuracy of an MNB 
classifier in the presence of large labeled datasets. As with other 
semi-supervised learning problems~\cite{zhu2009introduction}, these reductions 
in performance are due to violations of the model assumptions, previously 
stated in Sections~\ref{subsec:gen-models} 
and~\ref{subsec:multinomial-naive}.

\subsubsection{Extensions to the EM Algorithm: EM-$\lambda$}
\label{subsubsec:semi-super-em-ext}

Nigam et al.~\cite{Nigam2000} propose two extensions to the EM approach 
described in Section~\ref{subsec:semi-super-em} which attempt to cope with 
violations of some MNB model assumptions: the EM-Multiple and EM-$\lambda$ techniques. 
In the first case, the idea is to tackle violations of the 1:1 mixture 
component-to-topic correspondence assumption, allowing N:1 correspondences. The 
idea is that some topics may be separated into sub-topics (e.g. `football' and 
`cricket' in `sports'), with co-occurrences 
of words that may be better captured by multiple multinomial distributions. The 
unsupervised learning component of the problem is now detached from a particular 
number of `soft clusters', which may be now determined via cross-validation~\cite{Nigam2000}.\vertbreak

We describe the second case -- EM-$\lambda$ -- with more detail. Notice that 
when $|\mathcal{A}^{u}| \gg |\mathcal{A}^{\ell}|$, the influence of 
$\mathcal{A}^{\ell}$ in the maximization of expression~\ref{eq:log} 
is negligible, i.e. EM will be essentially be performing unsupervised 
clustering~\cite{Nigam2000}. The role of $\mathcal{A}^{\ell}$ would then 
be limited to provide the initial parameter estimates $\hat{\theta}$ and 
provide the number and topic correspondences for the `latent' variables of the 
mixture model. This may result in poor classification accuracy if the 
distribution of the data does not follow the GM's assumptions.\vertbreak

One solution is to reduce the influence of the unlabeled data in 
expression~\ref{eq:log} by a factor $\lambda$ with 
$0 \le \lambda \le 1$. The difference between EM-$\lambda$ and the method 
shown in Section~\ref{subsec:semi-super-em} is in the M-Step, with 
equations~\ref{eq:class-cond-estimate} and~\ref{eq:prior-estimate} altered to 
include the $\lambda$ factors:
\begin{equation}
\begin{split}
    \hat{\theta}_{w_k|c_j} &\equiv P(w_k|c_j,\hat{\theta})\\
    &= \frac{\alpha + \sum_{i=1}^{|\mathcal{A}|}\Lambda(i)N_{i,k}P(t_i = c_j|a_i)}{\alpha|\mathcal{D}| + \sum_{s=1}^{|\mathcal{D}|}\sum_{i=1}^{|\mathcal{A}|}N_{i,s}P(t_i = c_j|a_i)}
    \label{eq:class-cond-estimate-lambda}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
    \hat{\theta}_{c_j} &\equiv P(c_j|\hat{\theta}) = \frac{\alpha + \sum_{i=1}^{|\mathcal{A}|}\Lambda(i)P(t_i = c_j|a_i)}{\alpha|\mathcal{C}| + |\mathcal{A}^{\ell}| + \lambda|\mathcal{A}^{u}|}
    \label{eq:prior-estimate-lambda}
\end{split}
\end{equation}

where the function $\Lambda(i)$ defines the $\lambda$ weighing factor to apply, 
whether the an article $a_i$ belongs to $\mathcal{A}^{\ell}$ or 
$\mathcal{A}^{u}$:

\begin{equation}
    \Lambda(i) = \left\{ 
        \begin{array}{rl}
            \lambda &\mbox{ \text{if $a_i \in \mathcal{A}^{u}$}} \\
            1 &\mbox{ \text{if $a_i \in \mathcal{A}^{\ell}$}}
        \end{array} \right.
    \label{eq:1-final}
\end{equation}

If we consider the extreme values allowed for $\lambda$, when $\lambda = 0$ 
the influence of the unlabeled data is mitigated, resulting in a MNB classifier; 
with $\lambda = 1$ the contribution of the unlabeled dataset is maximum, 
corresponding to the basic MNB + EM approach described in 
Section~\ref{subsec:semi-super-em}. In~\cite{Nigam2000}, the value of 
$\lambda$ is chosen as that which maximizes the leave-one-out cross-validation 
classification accuracy.

